{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af3cdcf-e7c2-424d-a08d-888b941ebadb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Jenny Schilling (xdj3kg)\n",
    "## DS2002 Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a8301a9-fdb0-492d-914b-a322cdd5ef48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Putting it All Together: Data Integration & Analysis\n",
    "__Deliverable:__ Using Azure Databricks, design and populate a dimensional Data Lakehouse that represents a simple business process of your choosing. Examples might include retail sales, inventory, procurement, order management, transportation or hospitality bookings, medical appointments, student registration and/or attendance. You may select any business process that interests you, but remember that a dimensional Data Lakehouse provides for the post hoc summarization and historic analysis of business transactions that reflect the interaction between various entities (e.g., patients & doctors, retailers &\n",
    "customers, students & schools/classes, travelers & airlines/hotels).\n",
    "\n",
    "The most straight-forward approach is to identify an existing OLTP example database wherein all required data relationships already exist; however, you may choose to populate your Data Lakehouse using data from multiple sources as long as you can successfully use their business keys (e.g., customer code, product code) to establish the appropriate relationships between the Fact and Dimension tables. Your project should demonstrate your understanding of the differing types of relational data systems (OLTP/OLAP), and how data can be extracted from various source systems (structured, semi-structured, unstructured), transformed (cleansed, integrated), and then loaded into a destination system that’s optimized for post hoc diagnostic analysis. Your project should also demonstrate your knowledge of data integration patterns like ETL, ELT and ELTL, and architectures (e.g., lambda or kappa) for integrating batch and real-time (streaming) data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca09411e-a6f3-4a77-8202-c896ed014db3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Design Requirements:\n",
    "Your solution (database schema) needn’t be complex, but should meet the following requirements:\n",
    "- A Date dimension to enable the analysis of the business process over various intervals of time (the code for creating this in MySQL and Microsoft SQL Server has already been provided for you).\n",
    "- At least 3 additional dimension tables (e.g., customers, products, products)\n",
    "- At least 1 fact table that models the business process (e.g., sales, reservations, bookings)\n",
    "- Your solution must populate its dimensions using data originating from multiple sources:\n",
    "  - A relational database like Azure MySQL, or Azure SQL Server\n",
    "  - A NoSQL database like MongoDB Atlas, or Azure Cosmos DB\n",
    "  - Files (e.g., CSV) from a cloud-based file system; like the Databricks File System (DBFS)\n",
    "- Your solution must integrate datum of differing granularity (i.e., static and near real-time)\n",
    "- Your solution must include results that demonstrate the business value of your solution. For example, a query (SELECT statement) that summarizes transaction details by customer, product, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f786ac9d-46df-475f-859d-ffe320ea1eb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Functional Requirements:\n",
    "1. Your solution must demonstrate at least one batch execution (i.e., use sample source data \\[SQL, NoSQL and file system] to demonstrate loading at least one incremental data load).\n",
    "2. Your solution must demonstrate accumulating data that originates from a real-time (streaming) data source for a predetermined interval (mini-batch), integrating it with reference data, and then using the product as a source for populating your dimensional Data Lakehouse. (i.e., implement the Databricks bronze, silver, gold architecture).\n",
    "  a. Use the Spark AutoLoader to demonstrate integrating streaming data (using separate JSON files) for at least 3 intervals. This is most easily accomplished by segmenting the Fact table source data into 3 ranges and exporting them into 3 separate JSON files.\n",
    "  b. Illustrate the relationships between the “real-time” fact data and the static reference data. This is accomplished by joining fact and dimension tables at the Silver table phase.\n",
    "3. Use a Databricks Notebook to execute all data integration, object creation and query execution.\n",
    "4. Please submit all code, and other artifacts, in a GitHub repository in your account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c231eb1e-e6df-4cf1-8aa2-a4e3c246ad72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cbb1ac2-fe8c-4e60-9100-c90ce8599556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import pyspark.pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BinaryType\n",
    "from pyspark.sql.types import ByteType, ShortType, IntegerType, LongType, FloatType, DecimalType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aaa934f-e9fa-42c0-a28f-d7e014c1de35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae0ecbc6-0b3d-4c75-b17e-230ee2b28bb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_hostname = \"xdj3kg-ds2002-mysql.mysql.database.azure.com\"\n",
    "jdbc_port = 3306\n",
    "src_database = \"retail_sales_dw\"\n",
    "\n",
    "connection_properties = {\n",
    "  \"user\" : \"xdj3kg\",\n",
    "  \"password\" : \"Passw0rd123!\",\n",
    "  \"driver\" : \"org.mariadb.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "atlas_cluster_name = \"cluster0.2ihqfeb\"\n",
    "atlas_database_name = \"retail_sales_dw\"\n",
    "atlas_user_name = \"xdj3kg\"\n",
    "atlas_password = \"Passw0rd123!\"\n",
    "\n",
    "dst_database = \"retail_sales_dlh\"\n",
    "\n",
    "base_dir = \"dbfs:/FileStore/lab_data\"\n",
    "database_dir = f\"{base_dir}/{dst_database}\"\n",
    "\n",
    "data_dir = f\"{base_dir}/retail\"\n",
    "batch_dir = f\"{data_dir}/batch\"\n",
    "stream_dir = f\"{data_dir}/stream\"\n",
    "\n",
    "sales_stream_dir = f\"{stream_dir}/sales\"\n",
    "\n",
    "sales_output_bronze = f\"{database_dir}/sales_fact/bronze\"\n",
    "sales_output_silver = f\"{database_dir}/sales_fact/silver\"\n",
    "sales_output_gold   = f\"{database_dir}/sales_fact/gold\"\n",
    "\n",
    "dbutils.fs.rm(f\"{database_dir}/sales_fact\", True) \n",
    "\n",
    "dbutils.fs.rm(database_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adab9e5d-c425-4630-8b57-7686f76e966a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Define global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0c05068-a349-4793-92d8-66d241d1905f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_mongo_dataframe(user_id, pwd, cluster_name, db_name, collection, conditions, projection, sort):\n",
    "    '''Create a client connection to MongoDB'''\n",
    "    mongo_uri = f\"mongodb+srv://{user_id}:{pwd}@{cluster_name}.mongodb.net/{db_name}\"\n",
    "    \n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = client[db_name]\n",
    "    if conditions and projection and sort:\n",
    "        dframe = pd.DataFrame(list(db[collection].find(conditions, projection).sort(sort)))\n",
    "    elif conditions and projection and not sort:\n",
    "        dframe = pd.DataFrame(list(db[collection].find(conditions, projection)))\n",
    "    else:\n",
    "        dframe = pd.DataFrame(list(db[collection].find()))\n",
    "\n",
    "    client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "def set_mongo_collection(user_id, pwd, cluster_name, db_name, src_file_path, json_files):\n",
    "    '''Create a client connection to MongoDB'''\n",
    "    mongo_uri = f\"mongodb+srv://{user_id}:{pwd}@{cluster_name}.mongodb.net/{db_name}\"\n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    \n",
    "    '''Read in a JSON file, and Use It to Create a New Collection'''\n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(src_file_path, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "\n",
    "    client.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "310b20cb-2384-461d-bb44-21f7802c7418",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Populate Dimensions by Ingesting Reference (Cold-path) Data\n",
    "Create new databricks metadata database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa7fd4c-0831-490f-9480-3bb77977826b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP DATABASE IF EXISTS retail_sales_dlh CASCADE;\n",
    "CREATE DATABASE IF NOT EXISTS retail_sales_dlh\n",
    "COMMENT \"JS Capstone Project Database\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/retail_sales_dlh\"\n",
    "WITH DBPROPERTIES (contains_pii = true, purpose = \"DS2002 Capstone Project\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84064aa5-522c-4d63-933c-f29c6069296c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create new table that sources **date** dimension data from a table in Azure MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfacb238-e2e0-4830-9f62-4a863ce5310e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_date\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"jdbc:mysql://xdj3kg-ds2002-mysql.mysql.database.azure.com:3306/retail_sales_dw2\",\n",
    "  dbtable \"date_dim\",\n",
    "  user \"xdj3kg\",\n",
    "  password \"Passw0rd123!\"\n",
    ");\n",
    "\n",
    "USE DATABASE retail_sales_dlh;\n",
    "\n",
    "CREATE OR REPLACE TABLE retail_sales_dlh.date_dim\n",
    "COMMENT \"Date Dimension Table\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/retail_sales_dlh/date_dim\"\n",
    "AS SELECT * FROM view_date;\n",
    "\n",
    "DESCRIBE EXTENDED northwind_dlh.date_dim;\n",
    "SELECT * FROM northwind_dlh.date_dim LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382514bf-d260-4faf-b069-686f0adb047f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create new table that sources **customer** dimension data from a table in Azure MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a515a095-cca0-4c32-962c-db5fd1a88d41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_customer\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"jdbc:mysql://xdj3kg-ds2002-mysql.mysql.database.azure.com:3306/retail_sales_dw2\",\n",
    "  dbtable \"customer_dim\",\n",
    "  user \"xdj3kg\",\n",
    "  password \"Passw0rd123!\"\n",
    ");\n",
    "\n",
    "USE DATABASE retail_sales_dlh;\n",
    "\n",
    "CREATE OR REPLACE TABLE retail_sales_dlh.customer_dim\n",
    "COMMENT \"Customer Dimension Table\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/retail_sales_dlh/customer_dim\"\n",
    "AS SELECT * FROM view_customer;\n",
    "\n",
    "DESCRIBE EXTENDED northwind_dlh.customer_dim;\n",
    "SELECT * FROM northwind_dlh.customer_dim LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60652925-41d8-4fe4-a6af-226746014652",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create new table that sources **product** dimension data from a table in Azure MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbac9ed9-1a76-451b-a03d-65e049de64be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_product\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"jdbc:mysql://xdj3kg-ds2002-mysql.mysql.database.azure.com:3306/retail_sales_dw2\",\n",
    "  dbtable \"product_dim\",\n",
    "  user \"xdj3kg\",\n",
    "  password \"Passw0rd123!\"\n",
    ");\n",
    "\n",
    "USE DATABASE retail_sales_dlh;\n",
    "\n",
    "CREATE OR REPLACE TABLE retail_sales_dlh.product_dim\n",
    "COMMENT \"Product Dimension Table\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/retail_sales_dlh/product_dim\"\n",
    "AS SELECT * FROM view_product;\n",
    "\n",
    "DESCRIBE EXTENDED northwind_dlh.product_dim;\n",
    "SELECT * FROM northwind_dlh.product_dim LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61f5c5aa-9769-49ee-ae52-a9dcd0050aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fetch reference data from a NoSQL MongoDB Atlas Database\n",
    "This section fetches product review data from the MongoDB collection, and loads that data into the Data Lakehouse product dimension by the product key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "328b8c41-77a6-4337-8cd1-6329d438b6f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(batch_dir))  # '/dbfs/FileStore/lab_data/retail/batch'\n",
    "\n",
    "source_dir = '/dbfs/FileStore/lab_data/retail/batch'\n",
    "json_files = {\"reviews\" : 'retil_sales_dw.product_reviews.json'}\n",
    "\n",
    "set_mongo_collection(atlas_user_name, atlas_password, atlas_cluster_name, atlas_database_name, source_dir, json_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e66a99a6-4e5a-4a10-a39a-98ce80f3f070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import com.mongodb.spark._\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "val userName = \"xdj3kg\"\n",
    "val pwd = \"Passw0rd123!\"\n",
    "val clusterName = \"cluster0.2ihqfeb\"\n",
    "val atlas_uri = s\"mongodb+srv://$userName:$pwd@$clusterName.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "val df_reviews = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    ".option(\"spark.mongodb.input.uri\", atlas_uri)\n",
    ".option(\"database\", \"retail_sales_dw2\")\n",
    ".option(\"collection\", \"product_reviews\").load()\n",
    ".select(\"id\",\"product_key\",\"reviewer\",\"review_text\",\"rating\")\n",
    "\n",
    "display(df_reviews)\n",
    "df_reviews.printSchema()\n",
    "df_reviews.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"retail_sales_dlh.product_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3966872c-a1b8-4114-8cbe-8d97888c1a97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE DATABASE retail_sales_dlh;\n",
    "ALTER TABLE product_dim ADD COLUMN reviewer varchar(50);\n",
    "ALTER TABLE product_dim ADD COLUMN review_text text;\n",
    "ALTER TABLE product_dim ADD COLUMN rating INT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cce5b3b-8b85-4240-805d-10d531e80b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "spark_merged_df.write.jdbc(url=url, table=\"temporary_table_for_updates\", mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e44e095-1952-4ab3-bc81-f1f1a04fa748",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE product_dim pd\n",
    "JOIN temporary_table_for_updates tmp ON pd.product_key = tmp.product_key\n",
    "SET pd.reviewer = tmp.reviewer, pd.review_text = tmp.review_text, pd.rating = tmp.rating;\n",
    "\n",
    "DESCRIBE EXTENDED retail_sales_dlh.product_dim;\n",
    "\n",
    "SELECT * FROM retail_sales_dlh.product_dim LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94b5cae5-2e28-44d3-8b41-561c475558d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Including CSV files from a cloud-based file system: Databricks File System (DBFS)\n",
    "Adds data regarding whether each customer is a VIP from 'dbfs:/FileStore/lab_data/batch/customer_vip_info.csv' and aligns it with the first and last names in the customer dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18535bf2-8646-4d78-8582-31775efcf531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a8c9fe-4f39-46ae-9ae6-ad2956aba855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "customer_vip_data = f\"{batch_dir}/customer_vip_info.csv\"\n",
    "customer_vip_data.iloc[:, 2:] = customer_vip_data.iloc[:, 2:].astype(bool)\n",
    "\n",
    "df_vip = spark.read.format('csv').options(header='true', inferSchema='true').load(customer_vip_data)\n",
    "display(df_vip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1929adfb-dbe3-4d8b-a449-2e3758c29d6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE DATABASE retail_sales_dlh;\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.first_name,\n",
    "    c.last_name,\n",
    "    c.phone,\n",
    "    c.credit_limit,\n",
    "    v.is_vip,\n",
    "FROM\n",
    "    customer_dim AS c\n",
    "LEFT JOIN\n",
    "    df_vip AS v\n",
    "ON\n",
    "    c.first_name = v.first_name AND c.last_name = v.last_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784fa84a-eade-4ec3-9bdd-1a0aa0705880",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_vip.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"retail_sales_dlh.customer_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee52248f-f66f-4971-90ac-1ad811c32a97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED retail_sales_dlh.customer_dim;\n",
    "SELECT * FROM retail_sales_dlh.customer_dim LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a0eb40-bd4f-45a1-9f8d-93f70f3dc5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Integrating datum of differing granularity \n",
    "#### Sales Fact Table that models the business process\n",
    "Using autoloader to process streaming (hot path) sales fact data and implementing the Databricks bronze, silver, and gold architecture.\n",
    "\n",
    "#####Bronze Table: process 'raw' JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cce506d-db0e-42f3-a2a9-8924852d9599",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"json\")\n",
    " .option(\"cloudFiles.schemaLocation\", sales_output_bronze)\n",
    " .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    " .option(\"multiLine\", \"true\")\n",
    " .load(sales_stream_dir)\n",
    " .createOrReplaceTempView(\"sales_raw_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e458f6c4-26a6-4d89-b2bc-476f21dd6b01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_bronze_tempview AS (\n",
    "  SELECT *, current_timestamp() receipt_time, input_file_name() source_file\n",
    "  FROM sales_raw_tempview\n",
    ");\n",
    "\n",
    "SELECT * FROM sales_bronze_tempview;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcf1a067-9d4f-45fa-ac85-bea7cf3b429d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"sales_bronze_tempview\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{sales_output_bronze}/_checkpoint\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"sales_fact_bronze\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f91963-56e3-4ef5-93eb-adaa1769756e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Silver Table: include reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3533150-36c8-4190-8b78-e1caceade1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "  .table(\"sales_fact_bronze\")\n",
    "  .createOrReplaceTempView(\"sales_silver_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77f1e979-0bb7-4fb8-a291-9b862802cc4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sales_silver_tempview;\n",
    "DESCRIBE EXTENDED sales_silver_tempview;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8997de4-44aa-4596-ad65-3dd9708b326c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_fact_silver_tempview AS (\n",
    "  SELECT s.sale_id,\n",
    "    s.sale_date,\n",
    "    d.day_name_of_week AS sale_day_name_of_week,\n",
    "    d.day_of_month AS sale_day_of_month,\n",
    "    d.weekday_weekend AS sale_weekday_weekend,\n",
    "    d.month_name AS sale_month_name,\n",
    "    d.calendar_quarter AS sale_quarter,\n",
    "    d.calendar_year AS sale_year,\n",
    "    s.product_id,\n",
    "    p.product_name AS sale_product_name,\n",
    "    p.product_line AS sale_product_line,\n",
    "    p.quantity_in_stock AS sale_quantity_instock,\n",
    "    p.product_price AS sale_product_price,\n",
    "    p.reviewer AS sale_review,\n",
    "    p.review_text AS sale_review_text,\n",
    "    p.rating AS sale_rating,\n",
    "    s.customer_id,\n",
    "    c.first_name AS sale_customer_firstname,\n",
    "    c.last_name AS sale_customer_lastname,\n",
    "    c.phone AS sale_customer_phone,\n",
    "    c.credit_limit AS sale_customer_credit_limit,\n",
    "    c.is_vip AS sale_customer_isvip,\n",
    "    s.quantity_sold,\n",
    "    s.sales_amount\n",
    "  FROM sales_silver_tempview AS s\n",
    "  INNER JOIN retail_sales_dlh.customer_dim AS c\n",
    "  ON c.customer_id = s.customer_id\n",
    "  INNER JOIN retail_sales.product_dim AS p\n",
    "  ON p.product_id = s.product_id\n",
    "  INNER JOIN retail_sales_dlh.date_dim AS d\n",
    "  ON d.full_date = s.sale_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eea6ae24-308d-49d0-919d-a83140bba806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"sales_fact_silver_tempview\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{sales_output_silver}/_checkpoint\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"sales_fact_silver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb01097f-d9c1-403b-9875-4e1c4ab0cc5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sales_fact_silver;\n",
    "DESCRIBE EXTENDED retail_sales_dlh.sales_fact_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c32a326d-51b5-48b7-9f71-b8e57ec4ae86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Gold Table: perform aggregations using the CTAS approach\n",
    "This table includes the number of products sold per customer each month, along with the customer's ID, first & last name, and the month in which the sale was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d85b164-9974-41db-b5d5-cb50aae9aef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE retail_sales_dlh.fact_monthly_sales_by_customer_gold AS (\n",
    "  SELECT customer_id AS CustomerID\n",
    "    , sale_customer_lastname AS LastName\n",
    "    , sale_customer_firstname AS FirstName\n",
    "    , sale_month_name AS SaleMonth\n",
    "    , COUNT(product_id) AS ProductCount\n",
    "  FROM retail_sales_dlh.sales_fact_silver\n",
    "  GROUP BY CustomerID, LastName, FirstName, SaleMonth\n",
    "  ORDER BY ProductCount DESC);\n",
    "\n",
    "SELECT * FROM retail_sales_dlh.fact_monthly_sales_by_customer_gold;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1d6a54f-2dc6-46f1-bee1-c7c807527dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE retail_sales_dlh.fact_product_sales_by_customer_gold AS (\n",
    "  SELECT pc.CustomerID\n",
    "    , ss.sale_customer_lastname AS CustomerName\n",
    "    , ss.product_id AS ProductCode\n",
    "    , pc.ProductCount\n",
    "  FROM retail_sales_dlh.sales_fact_silver AS ss\n",
    "  INNER JOIN (\n",
    "    SELECT customer_id AS CustomerID\n",
    "    , COUNT(product_id) AS ProductCount\n",
    "    FROM retail_sales_dlh.sales_fact_silver\n",
    "    GROUP BY customer_id\n",
    "  ) AS pc\n",
    "  ON pc.CustomerID = ss.customer_id\n",
    "  ORDER BY ProductCount DESC);\n",
    "\n",
    "SELECT * FROM retail_sales_dlh.fact_product_sales_by_customer_gold;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65218e9c-43eb-455e-96ad-8e24a243a069",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###SQL query to demonstrate the business value\n",
    "This SQL query calculates the total sales quantity (using SUM), grouping by 'is_vip' and 'product_name' to see sales quantities for each product broken down by VIP status. I sort by total_quantity_sold in descending order to find out the top-selling products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "278386d2-3b10-404e-ac58-fc11c33a8343",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE retail_sales_dw;\n",
    "\n",
    "SELECT\n",
    "    c.is_vip,\n",
    "    p.product_name,\n",
    "    SUM(sales_fact.quantity_sold) AS total_quantity_sold\n",
    "FROM sales_fact\n",
    "JOIN customer_dim AS c ON sales_fact.customer_id = c.customer_id\n",
    "JOIN product_dim AS p ON sales_fact.product_id = p.product_id\n",
    "GROUP BY c.is_vip, p.product_name\n",
    "ORDER BY total_quantity_sold DESC; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c77bf6-4e51-405e-8506-90cfc93db810",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the file system\n",
    "%fs rm -r /FileStore/lab_data/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
